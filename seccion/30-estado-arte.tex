% !TeX root = ../tfm.tex
%! TEX root = ../tfm.tex

Dada la extensión del contexto trabajo previo sobre el que se asienta esta solución, esta sección se organiza en diversos bloques para facilitar su lectura y comprensión. En una primera sección analizaremos la historia y evolución de los sistemas de detección de caídas, introduciremos las principales familias y presentaremos el estado actual. Saltaremos a una nueva sección para introducir varias técnicas y modelos de aprendizaje automático que si bien están relacionadas con los modelos previamente introducidos es necesario exponer de forma aislada para poder justificar a lo largo de este desarrollo los componentes usados y decisiones tomadas. En una última sección de este capítulo trataremos la problemática de la optimización de modelos, técnicas y bibliotecas de desarrollo de modelos neuronales para sistemas embebidos.


\section{Detección de actividad humana y caídas}\label{sec:arte:detección_caídas}

Históricamente, los modelos de predicción de caídas están íntimamente ligados al estudio del reconocimiento de actividades humanas (\textit{HAR} del inglés \textit{Human Activity Recognition}): discernir a partir de los datos de diferentes sensores qué acción (\textit{AVC} por \textit{Actividad Vida Cotidiana}, o \textit{ADL} por sus siglas en inglés) estaba realizando el individuo. Según \citeA[p.10692]{Ozdemir2014} una caída o golpe no es más que otra actividad y deben considerarse una parte de este problema ya que "son eventos que suceden de forma involuntaria e inesperada durante la realización de las mismas [actividades]".

Los sistemas primigenios de detección de caídas se basaban en la capacidad del usuario de activar un mecanismo que a su vez activase algún tipo de alerta. Consciente de las limitaciones de estos sistemas, especialmente dada la naturaleza incapacitante para cualquier acción tras el impacto, \citeA{Williams1998} presenta un sistema automático, sin intervención del usuario, de detección de caídas graves. Con un sensor de impacto y otro de posición y un algoritmo analítico basado en cotas de los parámetros medidos, siendo uno de los primeros modelos analíticos de detección de actividad humana. Veremos en los siguientes puntos (\ref{sec:arte:modelos_analiticos},\ref{sec:arte:modelos_ml}) la evolución de estos sistemas.

Con la mejora de las capacidades de estos sistemas, empiezan a proliferar las implementaciones basadas en ellos. En \citeA{Anita2020} se recopilan los resultados de diferentes estudios que usan sistemas llevables para la detección de caídas,  las técnicas y modelos usadas a la vez que se establece unas directrices que debería cumplir todo sistema de detección de caídas:
\begin{itemize}
  \item No ser invasivo
  \item No restringir la movilidad del sujeto
  \item Tener baja latencia en la detección
  \item Ser capaz de distinguir caídas de otras actividades humanas cotidianas
\end{itemize}
\citeA{Anita2020} y otros autores \cite{Musci2020,Lim2014} introducen varias clasificaciones de estos sistemas. Si bien hay variaciones según los textos la mayoría coinciden en establecer una segregación según el tipo de algoritmo utilizado en \cita[{\citeNP[p.1]{Lim2014}}]{dos acercamientos principales, los basados en cotas o límites y los basados en aprendizaje automático}. También se definen diferentes sistemas según el tipo y situación de los sensores usados, aunque aquí es donde más variabilidad hay según el artículo, resulta de interés la categoría de los dispositivos \textit{llevables} definidos como \cita[{\citeNP[p.3]{Anita2020}}]{aquellos sistemas en los que los sensores usados para la detección se encuentran embebidos en un dispositivo que debe llevar puesto el sujeto, como por ejemplo en una pulsera}. Tanto para entender la evolución como para una mayor comprensión del desarrollo de este trabajo introduciremos las dos grandes familias de algoritmos de detección usadas.

\subsection{Modelos Analíticos: cálculos basados en cotas}\label{sec:arte:modelos_analiticos}

Los primeros acercamientos\cite{Williams1998,fallindex00,Chen2005,Kangas2008} al problema de la detección de caídas utilizaron modelos matemáticos basados en el análisis de diferentes parámetros físicos como la aceleración vertical, módulo de la aceleración y postura. A partir de todos estos parámetros, se establece una métrica o \textit{índice} que se calcula periódicamente. Según el algoritmo, se compara o analiza uno o varios índices con uno o varios niveles predeterminados, que de superarse indicaría que se ha producido una caída\cite{Luque2014}. En \citeA[p.~286-289]{Kangas2008} se presentan varios índices y algoritmos. Como observa \citeA{Bagala2012}, estos métodos puedan conseguir buenos resultados aplicados a pruebas de laboratorio o experimentos controlados, pero no en experimentos de uso real, en parte debido a un sobreajuste de la calibración de los valores al experimento, proceso que a su vez incremente la dificultad de configuración por parte de usuarios no expertos\cite{Kangas2008, Luque2014}. En general estos métodos contraponen sensibilidad a especificidad o a la inversa, como puede deducirse de los resultados de \citeA{Chen2005,Bourke2006,Kangas2008,Vilarinho2015} y verifica \citeA{Anita2020}. A pesar de ello, estos métodos siguen siendo ampliamente utilizados hoy en día por la baja complejidad computacional que requieren, lo que permite ser integrados en pequeños dispositivos poco obtrusivos y con bajas latencias, cumpliendo tres de los cuatro requisitos antes mencionados. Así mismo cabe destacar que ya en estos primeros estudios se recomienda evitar el uso de sensores en los brazos ya que \cita[{\citeNP[p.~3552]{Chen2005}}]{el frecuente y marcado movimiento del brazo durante la actividades cotidiana dificulta el uso de las fuerzas de aceleración medidas en el}.

Con la madurez, las lineas de investigación se centran en el impacto de la posición de los sensores sobre los resultados. Tal es el caso de \citeA{Bagala2012, Vilarinho2015, Luque2014}. Usan \textit{smartphones} o pulseras de actividad en diferentes posiciones para la captura de la aceleración con resultados muy dispares según el tipo de caída o el tipo de experimento. En el caso de \citeA{Vilarinho2015} a pesar de introducir las pulseras como instrumento de captura, delega en un equipo externo (en su caso un teléfono) el análisis y detección de caídas, de manera similar a los experimentos conducidos por \citeA{Luque2014} quien añade la posibilidad de realizar el cómputo en un servidor conectado a Internet.

A pesar de sus inconvenientes, su simplicidad y suficiente capacidad de detección permiten a estos algoritmos resultados similares o en algunos casos mejores a las de las soluciones comerciales existentes \cite{Kangas2008}. La evolución de estos índices sigue dos caminos aparentemente contradictorios: la simplificación o generalización y la mejora de la capacidad detectora del índice (entendemos por simplificación o generalización aquellos intentos por buscar algoritmos con mayor capacidad de generalización a la hora de detectar caídas en contraposición al uso de índices centrados en resolver un subconjunto concreto de estos eventos). Estos métodos se basan en establecer una métrica o \textit{índice} y buscar un  Partiendo del método usado por\citeA{Williams1998}, basada en establecer un \textit{índice de actividad} para el individuo a partir de su historial de caídas, género, edad, medicación, capacidades cognitivas, motoras y otros muchos parámetros, y un \textit{índice de predicción de caídas}. Este último parámetro es un contador del total de veces que se ha superado el índice de actividad en un determinado periodo que de rebasar un determinado nivel indicaría la posibilidad de que el sujeto hubiese sufrido una caída. Una última etapa en el algoritmo mide la actividad posterior al posible evento y de ser inferior a un segundo nivel, determinar que se ha producido una caída. 

Como veremos más adelante en el trabajo, cabe recalcar el acercamiento propuesto por \citeA{Bourke2006}, basado únicamente en el uso del módulo del vector aceleración capturado por un acelerómetro triaxial. En esta obra, \citeauthorNP{Bourke2006} determinan que la curva descrita por la evolución del módulo del vector aceleración (consistente en un valle al inicio de la caída sucedido de un pico en el momento del impacto) permite la segregación de diferentes actividades y de caídas estableciendo unos valores límite para estos valores valle y pico) este método tiene por definición una sensibilidad del 100\% aunque una muy baja especificidad \cite{Aziz2017,Bagala2012}. Posteriormente en \citeA{Bourke2008, Bourke2010} ajusta el algoritmo para añadir una etapa posterior de medición de actividad y postura consiguiendo métricas de precisión, sensibilidad y especificidad entre las mejores en esta categoría, añadiendo por contra cierta latencia en la alarma.
% ####################################################################

\iffalse

Sobre cálculos basados en cotas, tenemos \cite{fallindex00}, Kangas\cite{Kangas2008} lo compara con el uso ya sea de aceleración vertical o modulo de la aceleración y de nuevo obtiene resultados dispares según el tipo de caída simulada.

\todohide{
1- Monitorizado básico (usar módulo del vector aceleración y un threshold X )
2- Fall Index (Yoshida,  T.;  Mizuno,  F.;  Hayasaka,  T.;  Tsubota,  K.;  Wada,  S.;  Yamaguchi,  T.  A  Wearable  Computer System for a Detection and Prevention of Elderly Users from Falling. In Proceedings of  the  12th  International  Conference  on  Biomedical  and  Medical  Engineering  (ICBME),  Singapore, Singapore, 7–10 December 2005.  )
3- PerFallD (Dai, J.; Bai, X.; Yang, Z.; Shen, Z.; Xuan, D. PerFallD: A Pervasive Fall Detection System using Mobile   Phones.   In   Proceedings   of   the   8th   IEEE   International   Conference   on   Pervasive   Computing  and  Communications  Workshops  (PERCOM  Workshops),  Mannheim,  Germany,    29 March–2 April 2010; pp. 292–297) (usa también un giroscopio)
4- iFall (Sposaro,  F.;  Tyson,  G.  IFall:  An  Android  Application  for  Fall  Monitoring  and  Response.    In Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2009), Minneapolis, MN, USA, 2–6 September 2009; pp. 6119–6122. )}

\fi


\subsection{Modelos de Aprendizaje Automático}\label{sec:arte:modelos_ml}

Con el aumento en la capacidad de cálculo sumado al auge en investigación relacionado con la inteligencia artificial, aparecen los primeros estudios que aplican técnicas como \textit{K-Nearest Neighbours}, LSM (\textit{Least Square Method} o \textit{Método de Mínimos Cuadrados}), SVM, Detección Bayesiana, DTW y Redes Neuronales aplicadas a la detección de caídas. \citeA{Yang2010} analiza los resultados de aplicar \textit{SVM}, \textit{Redes Neuronales} y \textit{Naive Bayes} entre otros, con unos resultados muy prometedores. Especialmente para el caso de Naive Bayes con el que consigue una precisión del 88\%. Varios trabajos \cite{Luque2014,ReyesOrtiz2014,tfall,Ozdemir2014} prosiguen por esta vía estudiando en detalle algoritmos basados en Modelos Ocultos de Markov, SVM y Bayes, K-NN, llegando a obtener resultados cercanos al 100\% de precisión usando K-NN \cite{Ozdemir2014} y destacan la capacidad para generalizar el aprendizaje a casos no entrenados de SVM\cite{tfall, Aziz2017b}.

En los últimos años parecen destacar los estudios que usan algún tipo de red neuronal \cite{Musci2020,Shi2020,Casilari2020,Liu2020} para realizar la clasificación de actividades humanas y detección de caídas. Por ejemplo, \citeA{Shi2020} describe un método basado en acelerómetro triaxial llevado en la cintura para predecir caídas, tratando una caída como una actividad humana más y usando redes convolucionales; en \citeA[p.17]{Casilari2020} usa una red de 4 capas convolucionales para el mismo fin, y compara el rendimiento sobre multitud de bases de datos obteniendo que si bien se puede optimizar el rendimiento para un conjunto en concreto logrando resultados de precisión y especificidad del orden del 100\%, es muy costoso generar un modelo que generalice a todos los conjuntos de datos. Resultado verificado por \citeA{Luque2014,Aziz2017,Aziz2017b}.

Al ser una rama tan prolífica en resultados, aparecen metaestudios con el objetivo de aunar criterios y comparar resultados de los diferentes métodos propuestos como son las obras de \citeA{Anita2020} y \citeA{Aziz2017}. Estos estudios vuelven a poner de manifiesto la superioridad de los métodos basados en aprendizaje automático (con sensibilidades y especificidades superiores al 90\% en todos los algoritmos probados, frente al compromiso entre sensibilidad y especificidad que se da en los algoritmos analíticos, especialmente en los más simples como Bourke \cite{Aziz2017}.

De igual forma que pasó con sus hermanos los algoritmos basados en modelos analíticos\ref{sec:arte:modelos_analiticos} al llegar a la madurez los estudios empieza a poner énfasis en otros parámetros que impactan la efectividad de estos sistemas. Se centran en conocer la fisionomía de una caída para así poder mejorar los modelos. En \cite[p.~6]{Musci2020} se llega a la conclusión de que el uso de la información de la aceleración únicamente es una fuente de información suficientemente fiable para obtener métricas de detección satisfactorias usando una red neuronal recurrente. Analizando la duración y componentes de la señal de la aceleración durante una caída, \citeA[p.~16]{Casilari2020} llega a la conclusión que a partir de 5 segundos no hay ningún beneficio extra en aumentar el tamaño de la ventana. En \cite[p.~59]{Hassan2019} usan una ventana de 1s consiguiendo mejorar los clasificadores existentes (modelos basados en recurrentes) con un modelo basado en una red CNN-LSTM. Resultado que coincide con \cite[p.~2]{Liu2018} que acota la duración de una caída entre los 0,4 y 0,8 segundos, demostrando que una frecuencia de muestreo de 21,3Hz es suficiente cuando se utilizan modelos de aprendizaje automático. \citeA{Li2019} usa redes LSTM bidireccionales para realizar el análisis y captura contínua de actividad humana eliminando la necesidad de uso de una ventana y tratando los datos de actividad con técnicas de procesado de series temporales. Usando los datos obtenidos por un acelerómetro situado en la muñeca y un radar con un dispositivo de captura  mientras consigue mejorar los resultados obtenidos con SVM en similares cirtunstancias\cite[p.~9]{Li2019}. 


\subsection{Modelos basados en eventos o híbridos}\label{sec:arte:modelos_hybridos}

Si bien los algoritmos basados en aprendizaje automático consiguen estimadores muy precisos, adolecen de requerir sistemas con gran capacidad de cómputo y de grandes consumos energéticos, que son precisamente las grandes ventajas de los modelos analíticos. El uso de un sistema combinado es aparece descrito por \citeA[p.~9]{Aziz2017b} que recomienda usar una combinación de métodos basados en cotas, como generador de eventos, junto con técnicas de aprendizaje automático para obtener mejores y más eficientes estimadores. Sin embargo podemos observar un primer sistema de hibridación en \citeA{Lim2014}, quiienes proponen un método basado en una primera etapa de pre-detección usando un algoritmo basado en cotas para activar una segunda etapa. Esta segunda fase, usa la postura o ángulo del cuerpo para mediante un modelo HMM decidir si se trata de una caída o no. Llega a la conclusión de que obtiene \cita[{\citeNP[p.~5]{Lim2014}}]{Mejores resultados que con el método de cotas simple (\dots) y una fuerte reducción del coste computacional respecto al uso del modelo HMM únicamente}.

\citeA{Putra2017} refina la técnica usando un autómata de estados finitos para determinar a partir de de los resultados de varios índices analíticos si se debe activar el modelo basado en aprendizaje automático (frente al uso de enventanado fijo con o sin solapamiento (\textit{FOSW} y \textit{FNSW} respectivamente). Compara los resultados usando K-NN y SVM entre otros y obtiene, al igual que \citeauthorNP{Lim2014} que \cita[{\citeNP[p.~15]{Lim2014}}]{usando EvenT-ML arroja mejores resultados de precisión y F-score que usando FOSW y FNSW (...). Con la ventaja adicional que de requerir considerablemente menos capacidad de cálculo}. Confirma así la viabilidad de la técnica y como puede ayudar a mejorar la precisión de los sistemas basados puramente en aprendizaje automático.

\section{Predicción de series temporales: RNN}

De entre todas la técnicas de aprendizaje automático disponibles, este trabajo se centra en el uso de redes neuronales recurrentes. Introducidas por primera ven en el trabajo de \citeA{RNN1986} y designan toda una familia de topologías y celdas que presentan una capacidad de memoria temporal gracias al uso de la retroalimentación. Hoy en día las celdas más usadas son las \textit{LSTM} (\textit{Long Short-Term Memory}, celdas recurrentes que controlan el estado o memoria y su transmisión mediante unas entradas y salidas especiales llamadas puertas, \citeA{LSTM1997}) y una versión simplificada de estas: las GRU (\textit{Gated Recurrent Unit}, al igual que las LSTM tienen la capacidad de controlar la transmisión del estado o memoria a la etapa siguiente, pero con un número inferior de parámetros). Rápidamente se extiende su uso con muy buenos resultados en tratamiento del habla, traducción, reconocimiento de caracteres, segmentación, y en general tareas de tratamiento o predicción de series temporales.\todo{citar wikipedia o poner ejemplos de cada trabajo, dispo en wikipedia también, página inglesa claro}. Estas topologías de celda arrojan resultados comparables, incluso a veces con GRU superando a LSTM a pesar de su mayor simplicidad \cite{Chung2014,Su2018}.

Enseguida aparecen nuevas topologías de redes para mejorar o solventar algunos de los problemas. Por ejemplo, as topologías bidreccionales que aunan dos redes que recorren en sentidos contrarios las secuencias temporales que alimentan el modelo dando al modelo la capacidad de conocer tanto el contexto previo como el futuro \cite{Schuster1997}. Esta arquitectura además demuestra su superioridad en áreas donde se requiera mantener memoria a muy largo plazo, como por ejemplo la traducción de textos largos y tareas complejas de procesado del lenguaje \cite{Su2018}. Esta mayor capacidad de memoria temporal la aprovecha \citeA{Zhao2017} para mejorar la capacidad de detección de actividad humana en un 4,78\% respecto a redes recurrentes ordinarias.

%\subsection{Codificador-Decodificador}

\citeauthor{Cho2014} presentan la estructura \textit{codificador-decodificador} como: \cita[{\citeNP[p.~9]{Cho2014}}]{Una arquitectura de redes neuronales, capaz de comprender la relación entre dos secuencias de longitud arbitraria de dos conjuntos diferentes y evaluar la similitud de ambas en términos de probabilidad condicional o de proveer esa segunda secuencia a partir de a primera.}. Esta arquitectura arroja muy buenos resultados en los campos de traducción y generación de textos \cite{Cho2014,cho2014b,Serban2017,Tran2017} pero rápidamente pasa a aplicarse al campo de la detección de anomalías o problemas usando una medida de distancia entre la salida del modelo y la serie real para discernir\cite{Wang2018,malhotra2016,Park2018,Wang2020}. Nos interesa destacar los trabajo de \citeA{malhotra2016, Park2018} que mediante una red codificador-decodificador para regenerar la señal de entrada, de manera similar a un autocodificador, para posteriormente comparando la salida con la entrada calculando distancia euclídea poder discriminar anomalías e incluso predecir fallos en maquinaria mediante el análisis de señales temporales como el sonido.

  Esta misma arquitectura tiene una segunda virtud a explotar: la capacidad de realizar predicciones o reconstrucciones de varios pasos o muestras\cite{Peng2018}. La posibilidad de generar secuencias completas no solo simplifica la formulación de predicciones de varios pasos \cite{Kao2020}, sino que además muestra una excelente capacidad para encontrar correlaciones no lineales y realizar predicciones mejores que los métodos existentes hasta la fecha\cite{Peng2018,Du2019,Kao2020}. La combinación de la generación de secuencias de varios pasos a la vez así como sus aplicaciones en la detección de anomalías jugarán un papel clave en este trabajo.



\iffalse


\section{GRU vs LSTM para predicción de series}\label{sa_rnn}
\warn{corto y título de poco agrado: se esperaría una conclusión}

Respecto al uso de LSTM para detectar anomalías en series temporales, Wang \cite{Wang2020} \todo{sólo un artículo usando LSTM's?} usa LSTM para identificar anomalías en la señal de un motor (aunque el usa el error de recostrucción de la descomposición y recomposición wavelet de la señal como entrada a una triple red LSTM, nuestro enfoque es el contrario: usar LSTM a modo de transformada wavelet y luego comparar el error de recomposición).

En\cite{Qin2019} estudia el comportamiento de varias redes recurrentes para la predicción de la saturación de oxígeno en el agua y obtiene que las GRU son las que mejores predicciones (menos error) obtienen (Sobre LSTM e incluso RNN bidireccional). Kofi \cite{Koffi2020} compara LSTM y GRU para predecir mercado de valores con topologías muy variadas (número de celdas, de capas, stateless o no) y encuentra que GRU tiene mejor tasa de aciertos teniendo en cuenta el coste computacional (y muchas veces sin tenerlo) y que no siempre tener dos capas de RNN da mejores resultados.
\fi

