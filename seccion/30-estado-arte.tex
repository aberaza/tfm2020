%\documentclass[../tfm.tex]{subfiles}
%\begin{document}

Dada la extensión del contexto trabajo previo sobre el que se asienta esta solución, esta sección se organiza en diversos bloques para facilitar su lectura y comprensión. En una primera sección analizaremos la historia y evolución de los sistemas de detección de caídas, introduciremos las principales familias y presentaremos el estado actual. Saltaremos a una nueva sección para introducir varias técnicas y modelos de aprendizaje automático que si bien están relacionadas con los modelos previamente introducidos es necesario exponer de forma aislada para poder justificar a lo largo del desarrollo de esta obra diferentes componentes usados y decisiones tomadas. En una última sección de este capítulo trataremos la problemática de la optimización de modelos, técnicas y entornos de desarrollo para sistemas embebidos.


\section{Detección de actividad humana y caídas}\label{sec:arte:detección_caídas}

Históricamente, los modelos de predicción de caídas están íntimamente ligados al estudio del reconocimiento de actividades humanas (\textit{HAR} del inglés \textit{Human Activity Recognition}): discernir a partir de los datos de diferentes sensores qué acción (\textit{AVC} por \textit{Actividad Vida Cotidiana}, o \textit{ADL} por sus siglas en inglés) estaba realizando el individuo. Según \citeA[p.10692]{Ozdemir2014} una caída o golpe no es más que otra actividad y deben considerarse una parte de este problema ya que "son eventos que suceden de forma involuntaria e inesperada durante la realización de las mismas".

Los sistemas primigenios de detección de caídas se basaban en la capacidad del usuario de activar un mecanismo que a su vez activase algún tipo de alerta. Consciente de las limitaciones de estos sistemas, especialmente dada la naturaleza incapacitante para cualquier acción tras el impacto, \citeA{Williams1998} presenta un sistema automático, sin intervención del usuario, de detección de caídas graves. Con un sensor de impacto y otro de posición y un algoritmo analítico basado en cotas de los parámetros medidos, siendo uno de los primeros modelos analíticos de detección de actividad humana. Veremos en los siguientes puntos (\ref{sec:arte:modelos_analiticos},\ref{sec:arte:modelos_ml}) la evolución de estos sistemas.

Con la mejora de las capacidades de estos sistemas, empiezan a proliferar las implementaciones basadas en ellos. En \textit{A Survey on Recent Advances in Wearable Fall Detection Systems}\cite{Anita2020} se recopilan los resultados de las técnicas y modelos usadas a la vez que se establecen unas directrices que debería cumplir todo buen sistema de detección de caídas:
\begin{itemize}
  \item No ser invasivo
  \item No restringir la movilidad del sujeto
  \item Tener baja latencia en la detección
  \item Ser capaz de distinguir caídas de otras actividades humanas cotidianas
\end{itemize}
\citeA{Anita2020} y otros autores como \citeA{Musci2020} introducen varias clasificaciones de estos sistemas. Si bien hay variaciones según los textos la mayoría coinciden en establecer una segregación según el tipo de algoritmo utilizado en \cita[{\citeNP[p.1]{Lim2014}}]{dos acercamientos principales, los basados en cotas o límites y los basados en aprendizaje automático}. También se definen diferentes sistemas según el tipo y situación de los sensores usados, aunque aquí es donde más variabilidad hay según el artículo, resulta de interés la categoría de los dispositivos \textit{llevables} definidos como \cita[{\citeNP[p.3]{Anita2020}}]{aquellos sistemas en los que los sensores usados para la detección se encuentran embebidos en un dispositivo que debe llevar puesto el sujeto, como por ejemplo en una pulsera}. Tanto para entender la evolución como para una mayor comprensión del desarrollo de este trabajo introduciremos las dos grandes familias de algoritmos de detección usadas.

\subsection{Modelos Analíticos: cálculos basados en cotas}\label{sec:arte:modelos_analiticos}

Los primeros acercamientos\cite{Williams1998,fallindex00,Chen2005,Kangas2008} al problema de la detección de caídas utilizaron modelos matemáticos basados en el análisis de diferentes parámetros físicos como la aceleración vertical, módulo de la aceleración y postura. A partir de todos estos parámetros, se establece una métrica o \textit{índice} que se calcula periódicamente. Según el algoritmo, se compara o analiza uno o varios índices con uno o varios niveles predeterminados, que de superarse indicaría que se ha producido una caída\cite{Luque2014}. En \citeA[p.~286-289]{Kangas2008} se presentan varios índices y algoritmos. Como observa \citeA{Bagala2012}, estos métodos puedan conseguir buenos resultados aplicados a pruebas de laboratorio o experimentos controlados, pero no en experimentos de uso real, en parte debido por un sobreajuste de la calibración de los valores al experimento, proceso que a su vez incremente la dificultad de configuración por parte de usuarios no expertos\cite{Kangas2008, Luque2014}. En general estos métodos contraponen sensibilidad a especificidad o a la inversa, como puede deducirse de los resultados de \citeA{Chen2005,Bourke2006,Kangas2008,Vilarinho2015} y verifica \citeA{Anita2020}. A pesar de ello, estos métodos siguen siendo ampliamente utilizados hoy en día por la baja complejidad computacional que requieren, lo que permite ser integrados en pequeños dispositivos poco obtrusivos y con bajas latencias, cumpliendo tres de los cuatro requisitos antes mencionados.

Con la madurez, las lineas de inestigación se centran en el impacto de la posición de los sensores sobre los resultados. Tal es el caso de \citeA{Bagala2012, Vilarinho2015, Luque2014}. Usan smartphones o pulseras de actividad en diferentes posiciones para la captura de la aceleración con resultados muy dispares según el tipo de caída o el tipo de experimento. En el caso de \citeA{Vilarinho2015} a pesar de introducir las pulseras como instrumento de captura, delega en un equipo externo (en su caso un teléfono) el análisis y detección de caídas, de manera similar a los experimentos conducidos por \cite{Luque2014} quien añade la posibilidad de realizar el cómputo en un servidor conectado a internet.

A pesar de sus inconvenientes, su simplicidad y suficiente capacidad de detección permiten a estos algoritmos resultados similares o en algunos casos mejores a las de las soluciones comerciales existentes\cite{Kangas2008}. La evolución de estos índices sigue dos caminos aparentemente contradictorios: la simplificación o generalización y la mejora de la capacidad detectora del índice (entendemos por simplificación o generalización aquellos intentos por buscar algoritmos con mayor capacidad de generalización a la hora de detectar caídas en contraposición al uso de índices centrados en resolver un subconjunto concreto de estos eventos). Estos métodos se basan en establecer una métrica o \textit{índice} y buscar un  Partiendo del método usado por\citeA{Williams1998}, basada en establecer un \textit{índice de actividad} para el individuo a partir de su historial de caídas, género, edad, medicación, capacidades cognitivas, motoras y otros muchos parámetros, y un \textit{índice de predicción de caídas}. Este últimio índice es un contador del total de veces que se ha superado el índice de actividad en un determinado periodo que de superar un determinado nivel indicaría la posibilidad de que el sujeto hubiese sufrido una caída. Una última etapa en el algoritmo mide la actividad posterior al posible evento y de ser inferior a un segundo nivel, determinar que se ha producido una caída. 

Como veremos más adelante en el trabajo, cabe recalcar el acercamiento propuesto por \citeA{Bourke2006}, basado únicamente en el uso del módulo del vector aceleración capturado por un acelerómetro triaxial. \citeauthorNP{Bourke2006} determinan que la curva descrita por la evolución del módulo del vector aceleración (consistente en un valle al inicio de la caída sucedido de un pico en el momento del impacto) permite la segregación de diferentes actividades y de caídas estableciendo unos valores límite para estos valores valle y pico) este método tiene por definición una sensibilidad del 100\% aunque una muy baja especificidad \cite{Aziz2017,Bagala2012}. Posteriormente en \citeA{Bourke2008, Bourke2010} ajusta el algoritmo para añadir una etapa posterior de medición de actividad y postura consiguiendo métricas de precisión, sensibilidad y especificidad entre las mejores en esta categoría, añadiendo por contra cierta latencia en la alarma.
% ####################################################################

\begin{comment}

Sobre cálculos basados en cotas, tenemos \cite{fallindex00}, Kangas\cite{Kangas2008} lo compara con el uso ya sea de aceleración vertical o modulo de la aceleración y de nuevo obtiene resultados dispares según el tipo de caída simulada.

\todohide{
1- Monitorizado básico (usar módulo del vector aceleración y un threshold X )
2- Fall Index (Yoshida,  T.;  Mizuno,  F.;  Hayasaka,  T.;  Tsubota,  K.;  Wada,  S.;  Yamaguchi,  T.  A  Wearable  Computer System for a Detection and Prevention of Elderly Users from Falling. In Proceedings of  the  12th  International  Conference  on  Biomedical  and  Medical  Engineering  (ICBME),  Singapore, Singapore, 7–10 December 2005.  )
3- PerFallD (Dai, J.; Bai, X.; Yang, Z.; Shen, Z.; Xuan, D. PerFallD: A Pervasive Fall Detection System using Mobile   Phones.   In   Proceedings   of   the   8th   IEEE   International   Conference   on   Pervasive   Computing  and  Communications  Workshops  (PERCOM  Workshops),  Mannheim,  Germany,    29 March–2 April 2010; pp. 292–297) (usa también un giroscopio)
4- iFall (Sposaro,  F.;  Tyson,  G.  IFall:  An  Android  Application  for  Fall  Monitoring  and  Response.    In Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2009), Minneapolis, MN, USA, 2–6 September 2009; pp. 6119–6122. )}

\end{comment}


\subsection{Modelos de Aprendizaje Automático}\label{sec:arte:modelos_ml}

Con el aumento en la capacidad de cálculo sumado al auge en investigación relacionado con la inteligencia artificial, aparecen los primeros estudios que aplican técnicas como \textit{K-Nearest Neighbours}, LSM (\textit{Least Square Method} o \textit{Método de Mínimos Cuadrados}), SVM, Detección Bayesiana, DTW y Redes Neuronales aplicadas a la detección de caídas. \citeA{Yang2010} analiza los resultados de aplicar \textit{SVM}, \texit{Redes Neuronales} y \textit{Naive Bayes} entre otros con unos resultados muy prometedores, especialmente para el caso de Naive Bayes con el que consigue una precisión del 88\%. Varios trabajos \cite{Luque2014,ReyesOrtiz2014,tfall,Ozdemir2014} prosiguen por esta vía estudiando en detalle algoritmos basados en Modelos Ocultos de Markov, SVM, Bayes, K-NN, llegando a obtener resultados cercanos al 100\% de precisión usando K-NN \cite{Ozdemir2014} y destacar la capacidad para generalizar el aprendizaje a casos no entrenados de SVM\cite{tfall, Aziz2017b}.

En los últimos años parecen destacar los estudios que usan algún tipo de red neuronal \cite{Musci2020,Shi2020,Casilari2020,Liu2020} para realizar la clasificación de actividades humanas y detección de caídas. Por ejemplo, \citeA{Shi2020} describe un método basado en acelerómetro triaxial llevado en la cintura para predecir caidas, tratando una caida como una actividad humana más y usando redes convolucionals; en \citeA[p.17]{Casilari2020} usa una red de 4 capas convolucionales para el mismo fin, y compara el rendimiento sobre multitud de datasets obteniendo que si bien se puede optimizar el rendimiento para un conjunto en concreto logrando resultados de precisión y especificidad del orden del 100\%, es muy costoso generar un modelo que generalice a todos los datasets. Resultado verificado por \citeA{Luque2014,Aziz2017,Aziz2017b}.

Al ser una rama tan prolífica en resultados, aparecen metaestudios con el objetivo de aunar criterios y comparar resultados de los diferentes métodos propuestos como son las obras de \citeA{Anita2020}, \citeA{Aziz2017}. \todo{falta chicha y mejorar redacción}Finalmente, en \cite{Anita2020} tenemos un resumen con los últimos avances y resultados. Introduce otras fuentes de información como es el riesgo biológico, o predisposicion a la caída debido a la edad y otros factores de deterioro de la salud.

De igual forma que pasó con sus hermanos los algoritmos basados en modelos analíticos\ref{sec:arte:modelos_analiticos} al llegar a la madurez los estudios empieza a poner énfasis en otros parámetros que impactan la efectividad de estos sistemas. Se centran en conocer la fisionomía de una caída para así poder mejorar los modelos. En \cite[p.6]{Musci2020} se llega a la conclusión de que el uso de la información de la aceleración únicamente es una fuente de información suficientemente fiable para obtener métricas de detección satisfactorias usando una red neuronal recurrente. Analizando la duración y componentes de la señal de la aceleración durante una caída, \citeA[p.16]{Casilari2020} llega a la conclusión que a partir de 5 segundos no hay ningún beneficio extra en aumentar el tamaño de la ventana. En \cite[p.59]{Hassan2019} usan una ventana de 1s consiguiendo mejorar los clasificadores existentes. Resultado que coincide con \cite[p.2]{Liu2018} que acota la duración de una caída entre los 0,4 y 0,8 segundos, demostrando que una frecuencia de muestreo de 21,3Hz es suficiente cuando se utilizan modelos de aprendizaje automático.  


\subsection{Modelos basados en eventos o híbridos}\label{sec:arte:modelos_hybridos}

Si bien los algoritmos basados en aprendizaje automático consiguen estimadores muy precisos, adolecen de requerir sistemas con gran capacidad de cómputo y de grandes consumos energéticos, que son precisamente las grandes ventajas de los modelos analíticos. Este enfoque es sugerido por \cite[p.~9]{Aziz2017b} que recomienda usar una combinación de métodos basados en cotas junto con técnicas de aprendizaje automático para obtener mejores estimadores. Con la idea de aunar lo mejor de ambos modelos, \citeA{Lim2014} propone un método basado en una primera etapa de pre-detección usando un algoritmo basado en cotas para activar una segunda etapa usando la postura o ángulo del cuerpo para usando HMM para decidir si se trata de una caída o no. Llega a la conclusión de que obtiene \cita[{\citeNP[p.5]{Lim2014}}]̣{Mejores resultados qqe con el método de cotas simple (...) y una fuerte reducción del coste computacional respecto al uso del modelo HMM únicamente}.

\citeA{Putra2017} refina la técnica usando un autómata de estados finitos para determinar a partir de de los resultados de varios índices analíticos si se debe activar el modelo basado en aprendizaje automático (frente al uso de enventanado fijo con o sin solapamiento (\textit{FOSW} y \textit{FNSW} respectivamente}. Compara los resultados usando K-NN y SVM entre otros y obtiene, al igual que \citeauthorNP{Lim2014} que \cita[{\citeNP[p.~15]{usando EvenT-ML arroja mejores resulcados de precisión y F-score que usando FOSW y FNSW (...). Con la ventaja adicional que de requerir considerablemente menos capacidad de cálculo}]. Confirma así la viabilidad de la técnica y como puede ayudar a mejorar la precisión de los sistemas basados puramente en ML.


\section{Predicción de series temporales: RNN}

De entre todas la técnicas de aprendizaje automático disponibles, este trabajo se centra en el uso de redes neuronales recurrentes.  


  \begin{comment}
\warn{muy corto}

El uso de redes neuronales recurrentes para la predicción de series temporales es ampliamente respondido hoy en dia \todo{citar estudios}. Basaremos la etapa de clasificación en la capacidad de predecir episodios conocidos de este tipo de redes.

\section{GRU vs LSTM para predicción de series}\label{sa_rnn}
\warn{corto y título de poco agrado: se esperaría una conclusión}

Respecto al uso de LSTM para detectar anomalías en series temporales, Wang \cite{Wang2020} \todo{sólo un artículo usando LSTM's?} usa LSTM para identificar anomalías en la señal de un motor (aunque el usa el error de recostrucción de la descomposición y recomposición wavelet de la señal como entrada a una triple red LSTM, nuestro enfoque es el contrario: usar LSTM a modo de transformada wavelet y luego comparar el error de recomposición).

En\cite{Qin2019} estudia el comportamiento de varias redes recurrentes para la predicción de la saturación de oxígeno en el agua y obtiene que las GRU son las que mejores predicciones (menos error) obtienen (Sobre LSTM e incluso RNN bidireccional). Kofi \cite{Koffi2020} compara LSTM y GRU para predecir mercado de valores con topologías muy variadas (número de celdas, de capas, stateless o no) y encuentra que GRU tiene mejor tasa de aciertos teniendo en cuenta el coste computacional (y muchas veces sin tenerlo) y que no siempre tener dos capas de RNN da mejores resultados.

Aplicado al problema de la predicción de la actividad humana, Li\cite{Li2019} \todo{de nuevo... pocas referencias a estudios que usen LSTMs y rnn y grus} usa redes con varias capas LSTM bidireccionales usando los datos obtenidos por un acelerómetro y un radar con un dispositivo de captura en la muñeca mientras consigue resultados comparables a las mejores técnicas de clasificación existentes.

Para la predicción \textit{many-to-many} de señales temporales, Cho \cite{cho2014} introduce el concepto de Codficador-Decodificador alpicado a la traducción automática. Esta técnica usa un espacio de representación intermedio, una capa de conversión del espacio original al intermedio y finalmente otra de interpretación desde el espacio intermedio al final. Similar a como funcionan los autoencoders o los embeddings.

\end{comment}

\section{Pruning y técnicas para reducir complejidad de modelos}\label{sec:arte:optimizacion}

Si bien hemos visto cómo podemos apoyarnos CREO QUE VOY A MOVER ESTO A PRERREQUISITOS!


\begin{comment}
A pesar de la creciente capacidad de cálculo de los dispositivos vestibles, será necesario optimizar el rendimiento del modelo con el fin de conseguir la mayor capacidad de predicción por Hz del sistema. Para ello dispondremos de dos técnicas: \textit{Pruning} y \textit{Cuantificación}.

\subsection{Pruning}
Un modelo de redes neuronales se puede entender como una matriz de pesos que indican la contribción de cada nodo o neurona al resultado final. Es normal que en esa red haya nodos que aporten más información que otros. Este efecto no es necesariamente negativo, la regularización o normalización L1 busca precisamente este efecto para reducir el sobreajuste de la red a los datos de entrenamiento.

Esta diferencia de aportación nos permite realizar una compresión del modelo. Si aceptamos sufrir unas ciertas pérdidas en la capacidad predictiva del mismo, podemos eliminar los nodos de menor peso reduciendo el tamaño de la red y su consumo de memoria. Esta reducción permite además acelerar el modelo y adaptarlo al uso en sistemas embebidos.\todo{Buscar bibliografía formal para: https://www.machinecurve.com/index.php/2020/09/23/tensorflow-model-optimization-an-introduction-to-pruning/}

\todo{Este artículo cita referencias y es ligeramente más técnico.
https://www.machinecurve.com/index.php/2020/09/29/tensorflow-pruning-schedules-constantsparsity-and-polynomialdecay/}

Una de las técnicas más sencillas para realizar esta eliminación de nodos consiste en definir una aportación mínima objetivo y comparar la magnitud de la aportación individual al resultado final, eliminando aquellos que no alcanzan el nivel $\lambda$ deseado
\[
\hat{w_i} = \left\{ \begin{matrix} w_i & :\mbox{si} |w_i|>\lambda\\
  0 & :\mbox{si} |w_i|\leq\lambda\end{matrix} \right.
\]

Esta compresión se puede hacer sobre un modelo ya entrenado (siendo recomendable realizar un nuevo proceso de entrenamiento para balancear los pesos de los nodos existentes y reducir el error introducido) o realizarlo durante el entrenamiento como si de una regularización más se tratara.

En el caso de TensorFlow\todo{referencia!!!!}, el entorno usado para este proyecto, se permite realizar esta regularización durante el entrenamiento de dos formas:

\begin{itemize}
\item Eliminando nodos de forma lineal, o a tasa constante
\item Eliminando nodos de forma polinomial, o a tasa creciente
\end{itemize}
\warn{si lo introduzco y lo uso, debo tratar ventajas e inconvenientes}

\subsection{Cuantificación}
\warn{citas necesarias}
Entendemos por cuantificación al proceso de discretizar un grupo de valores. En este caso hace referencia precisamente al descenso en la precisión numérica usada para representar los pesos de nodos y valores de entrada y salida de una red neuronal.

La cuantificación de los valores de entrada y salida a la red consiste en reducir el peso de estos valores. Normalmente se normaliza la entrada y se convierte en valores de tipo entero. Tras el paso por la red, a la salida se aplicac la operación inversa para escalar la salida al rango de valores esperado.

Por su parte la cuantificación de la red neuronal realiza una operación similar con los pesos de los enlaces de las neuronas. Reduce la precisión de las representaciones de estos valores consiguiendo un ahorro en recursos y una mejora en tiempo de cálculo al eliminar el requisito de usar unidades de coma flotante en sistemas embebidos.


\end{comment}
%\end{document}

