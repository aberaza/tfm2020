% !TeX root = ../tfm.tex
%! TEX root = ../tfm.tex

\iffalse
info{Indicar el trabajo previo realizado para guiar el desarrollo del software.
Debe identificar adecuadamente el problema a tratar, contexto adecuado de uso y funcionamiento de la aplicación. Idealmente se debería realizar con expertos en la materia a tratar.}

todo{referenciar los requisitos de un sistema de detección de caídas, en detalle si es posible}
\fi

En este capítulo presentaremos al lector una serie de desarrollos previos sobres que serán utilizados o sobre los que se asentará el sistema de detección de caídas. Esta selección es fruto de las decisiones tomadas tras una etapa de estudio del estado de la tecnología en el área de detección de actividad humana y análisis de diferentes opciones de materiales, servicios, técnicas, modelos y datos disponibles. Todos los desarrollos aquí presentes fueron seleccionados a priori, antes de iniciar cualquier otra etapa de desarrollo, basándonos únicamente en los requisitos y necesidades del sistema. En los capítulos sucesivos se hará mención y justificación del uso o ausencia en la implementación final de todas estas herramientas.

\section{Plataforma y Entorno de Desarrollo}\label{sec:req:plataforma}

Tras adquirir información de entorno y con el fin de acotar y restringir las opciones disponibles de cara a la elaboración del algoritmo a emplear es necesario determinar sobre qué plataforma o dispositivo va a ejecutarse la aplicación. Esta decisión impondrá nuevas restricciones tales como tipos de fuentes de información/sensores disponibles, frecuencias de muestreo, precisión y magnitud, posición del dispositivo, entorno de programación a utilizar.

Entre los requisitos que debe cumplir el sistema destacamos principalmente:

\begin{itemize}
  \item Autónomo: Integrar en una única unidad la capacidad de adquisición, cómputo y alerta.
  \item No ser obtrusivo: El dispositivo no debe interferir o limitar la actividad cotidiana del usuario final.
  \item Programable: El dispositivo debe permitir la ejecución de código de terceros, no debe ser una plataforma cerrada.
  \item Capaz de ejecutar modelos de redes neuronales/aprendizaje automático.
\end{itemize}

Se tendrán en cuenta además de cara al cribado final otros criterios como el coste y la preferencia del autor sobre los entornos de programación a usar así como la adaptación al público objetivo: personas mayores de 65 años. Este grupo de población es más reacio al uso de sistemas computerizados por no estar familiarizados con su uso. Así se buscará ofrecer un sistema que reemplace un objeto de uso cotidiano, con una experiencia de usuario equiparable a la de este objeto y que evite cualquier tipo de interacción que no existiera previamente para la manipulación del objeto original.

\subsection{Dispositivo físico}\label{sub:req:hardware}

Analizamos en primera instancia los

\subsubsection{Tipos de dispositivos analizados}

\paragraph{Teléfono} o \textit{Smartphone}. Un aparato que se ha vuelto ubicuo en nuestra sociedad. Con gran capacidad de cálculo y multitud de sensores. Según la firma \textit{Kantar}(\url{https://www.kantarworldpanel.com/global/smartphone-os-market-share/}) el 99,99\% de los teléfonos vendidos en españa en los 9 primeros meses de 2020 fueron smartphones con IOS o Android y por ende fácilmente programables y con soporte para ejecutar múltiples entornos de modelado de sistemas de aprendizaje automático. Sin embargo adolecen de una complejidad extra: no es un dispositivo que se lleve vestido, y es fácil que el usuario final no lo lleve siempre consigo. Además, en los grupos de poblaciones de edad muy avanzada donde su uso no está generalizado, se convierte en un elemento ajeno y que puede generar rechazo o olvido en su uso.

\paragraph{Chalecos, cinturones y bandas} y otras prendas de vestir han sido usadas con anterioridad \cite{Liu2020, MobiFall} para situar los sensores. La falta de soluciones comerciales disponibles obligaría a desarrollar una plataforma personalizada y por tanto adaptada a los requisitos. Sin embargo este acercamiento se considera altamente intrusivo tanto por limitar al usuario en su vestimenta así como por servir únicamente cuando el usuario lleve puesto el complemento de ropa especificado. Además, la manipulación del mismo no es evidente, pues requiere de una manipulación muy diferente a la prenda de vestir original.

\paragraph{Pulseras de actividad y Relojes inteligentes}. Hace tiempo que las podómetros evolucionaron gracias a la mejora en la miniaturización de sensores, baterías y microprocesadores en sistemas avanzados de medida no ya de la cantidad de pasos sino de otros muchos parámetros como pulso, oxígeno en sangre, temperatura corporal y orientación. Por su parte, el paso de los relojes computadoras interconectadas y programables se ha realizado más recientemente. Actualmente puede resultar difícil distinguir en la oferta disponible entre un \textit{smartwarch} (reloj inteligente) y una pulsera. Muchas veces es la propia definición del fabricante la que especifica su tipo, por criterios tan dispares como la producción tradicional de la empresa (así pues, Fossil llamará a sus productos relojes inteligentes, mientras otro dispositivo de similares características y formato de la marca FitBit será llamado pulsera por la marca). Al existir tal variedad de oferta es fácil encontrar dispositivos que cumplan con los requisitos de sensores, capacidad de cálculo y de ser programado, así como soporte para entornos de aprendizaje automático. Al tener apariencia de un reloj de pulsera, resultan un soporte ideal: son dispositivos de uso cotidiano, con una manipulación sencilla, y que se llevan siempre puestos. Con el interés de buscar un sistema lo más transparente de cara al usuario posible nos decantamos por esta opción, a pesar de conocer las limitaciones del uso de la muñeca como fuente de información de la aceleración para la detección de actividades \cite{Chen2005} .

\subsubsection{Relojes Inteligentes WatchOS}

A la hora de definir las familias en las que subdividir este tipo de dispositivos encontramos que por norma general cada fabricante ha decidido crear su plataforma única. Tan solo el sistema operativo WearOS parece ser utilizado por varios fabricantes diferentes. No descartamos la opción de implementar una solución propietaria, por lo que también analizamos algunas de los sistemas de microcontroladores más extendidos.

% \tablan{tab:familias_smartband}{Familias de Relojes Inteligentes y Pulseras de Actividad}{llll}{
\tablan{tab:familias_smartband}{Familias de Relojes Inteligentes y Pulseras de Actividad}{p{0.20\textwidth}p{0.1\textwidth}p{0.20\textwidth}p{0.35\textwidth}}{
  \emph{Plataforma} & \emph{Progr.} & \emph{Soporte AA} & \emph{Notas} \\ \midrule
  Garmin     & No & No &  \\
  FitBit     & Si & No & \\
  Huawei     & No & No & \\
  TizenOS(Samsung) & Si & \makecell[l]{uTensorfFlow-lite} & Puede soportar otros entornos como Caffe2, PyTorch pero han de ser portados por el desarrollador \\
  WatchOS(Apple) & Si & \makecell[l]{CoreML\tnote{1}} & PyTorch, Tensorflow, Caffe, LibSVM y otros modelos pueden ser importados a CoreML. \\
  Arduino/ESP32 & Si & \makecell[l]{TensorFlow-lite} & Muy baja potencia de cálculo y memoria impiden la ejecución de modelos complejos. \\
  WearOS(Google)  & Si & \makecell[l]{TensorfFlow-lite\\Caffe2\\NNAPI\tnote{1}} & Mediante NNAPI permite realizar cómputos intensivos. \\
}{
  \item [1] API para cálculo intensivo que permite portar bibliotecas de Aprendizaje Automático
}{2}

Comparando los requisitos con los resultados de la tabla \ref{tab:familias_smartband} descartamos en primera instancia los dispositivos Garmin, FitBit y Huawei por no permitir el desarrollo de aplicaciones que usen modelos aprendizaje automático. Aunque en teoría sí hay soporte para usar modelos entrenados en las plataformas ESP32/Arduino, al disponer de cantidades de memoria y procesadores poco potentes su uso real queda muy limitado, prácticamente restringido a una mera curiosidad.

De los tres grupos restantes de dispositivos, la familia de dispositivos WatchOS queda relegada a la tercera posición por su alto precio (hasta 5 veces más caros que los dispositivos de la competencia). La decisión final se decanta finalmente por usar dispositivos WearOS por la preferencia y conocimiento de la plataforma del que ya dispone el autor.

\subsection{Entorno de Desarrollo}

Nos referimos por entorno de desarrollo al conjunto de herramientas usadas para implementar el sistema. Tanto para el la generación, entrenamiento y análisis de modelos como para la implementación y adaptación del sistema de detección de caídas.



\subsection{Soporte en la Nube}\label{sec:req:nube}

Si bien el sistema a desarrollar es autocontenido, preveemos la necesidad de implementar una infraestructura de servidor en la nuve para ampliar la funcionalidad y permitir la recopilación de datos, especialmente durante la etapa de recopilación de muestras para crear la base de datos. En concreto buscamos una plataforma de servicios que permita:

\begin{itemize}
  \item Implementar servicios/APIs de forma sencilla.
  \item Sin necesidad de infraestructura para reducir el mantenimiento.
  \item Permita almacenar grandes cantidades de datos.
\end{itemize}

Con estas necesidades en mente seleccionamos la plataforma de servicios en la nube de \textit{Amazon AWS}(\url{https://aws.amazon.com}) dado que además de cumplir con los requisitos mínimos, su amplia popularidad ha permitido crear una comunidad que ofrece un amplio soporte. De toda la oferta de servicios disponibles nos centraremos en el servicio de almacenamiento \textit{S3}, \textit{AWS lambda} para implementar servicios \textit{sin servidor} (sin infraestructura dedicada) y \textit{Amazon API Gateway} para exponer los servicios en Internet. Como extra a valorar cabe destacar la gran cantidad de lenguajes de programación soportados por AWS lambda, lo cual facilitará la implementación al no requerir aprender nuevos lenguajes de programación.

\section{Bases de datos de actividad}\label{sec:req:bases:datos}

Tras la elección tanto de la plataforma física, entorno de desarrollo así como del algoritmo usado, iniciamos la búsqueda de una base de datos que cumpla con los criterios siguientes:

\begin{enumerate}
  \item Contenga datos de la aceleración. Idealmente las 3 componentes espaciales por separado.
  \item Contenga datos tanto de caídas como de otras actividades cotidianas.
  \item La medida de la aceleración se toma desde un sensor en la muñeca.
  \item Contenga resultados de varios sujetos
\end{enumerate}

Tras analizar como se ve en la \autoref{tab:bases_datos} varias bases de datos relacionadas con la detección de actividades cotidianas y caídas. Por la localización de los sensores de medida, la única que cumple los requisitos es UMAFall, sin embargo tiene dos problemas:
\tablan{tab:bases_datos}{Bases de datos de actividad humana}{p{0.2\textwidth}ccccm{0.2\textwidth}m{0.2\textwidth}}{
  \emph{BBDD}    & \emph{Sujetos} & \emph{AHC} & \emph{Caídas} & \emph{Muestras} & \emph{Sensores}      & \emph{Posición} \\ \midrule
  MobiFall\tnote{1}& 24      & 9   & 4      & 630      & Acelerómetro\newline Giroscopio & Pantalón \\
  MobiAct\tnote{2}& 57      & 9   & 4      & 2526     & Acelerómetro  & Pantalón \\
  tFall\tnote{3}  & 10      & *   & 8      & 10909     & Acelerómetro  & Bolsillo\newline Bolso \\
  SisFall\tnote{4}& 38      & 19  & 15     & 4505      & Acelerómetro\newline Giroscopio & Cintura \\
  UMAFall\tnote{5}& 17      & 8   & 3      & 531       & Acelerómetro\newline Giroscopio\newline Magnetómetro & Pecho\newline Cintura\newline Pantalón\newline Tobillo\newline Muñeca \\
}{
  \item [1]\citeA{MobiFall}
  \item [2]\citeA{MobiAct}
  \item [3]\citeA{tfall}
  \item [4]\citeA{Sucerquia2017}
  \item [5]\citeA{Edu/UMA/2017}
  }{2}
\begin{itemize}
  \item Escasez de eventos: tan solo 531 eventos, de 25 categorías reducen el número de instancias por categorías a un promedio de 21. Claramente insuficiente para obtener una cifra significativa.
  \item Poco extendida entre la comunidad. En parte por ser la compilación más reciente, no hemos encontrado estudios suficientes que utilizasen UMAFall para poder realizar una comparación de resultados.
\end{itemize}
Concluimos esta etapa considerando desierta la oferta disponible. Tomamos la decisión de dividir la tarea a realizar en dos ejercicios:
\begin{enumerate}
  \item Implementar, validar, y comparar la viabilidad del algoritmo (independientemente de la plataforma elegida).
  \item Portar el modelo y entrenarlo para la plataforma escogida.
\end{enumerate}

Esta separación de objetivos permite reducir los requisitos de las bases de datos necesarios para cada uno tal y como recogemos a continuación.


\subsection{Bases de Datos para estudiar el algoritmo}

Al querer únicamente estudiar cómo se comporta el modelo diseñado, no es exigencia la restricción impuesta por el uso de un reloj o pulsera como dispositivo de captura de que los datos sean medidos en la muñeca. Por tanto, estudiando de nuevo la tabla \todo{crear la tabla y poner la referencia} observamos que ahora sí que hay varios candidatos viables.

%SisFALL (Tras considerar otros muchos como umafall, mobiact/mobifall, etc): mayor cantidad de caídas y muchas de las actividades son consideradas \textit{quasi-caidas} y por tanto son casos límite. Si el modelo es capaz de dar buenos resultados en estas condiciones, es un buen modelo. Además, es usado por \cite{Musci2020} y varios de los estudios presentes en \cite{Anita2020} por lo que resulta ideal para comparar los resultados.

\subsection{Base de Datos para la implementación final}

Tras validar el funcionamiento y compararlo con otros modelos en igualdad de condiciones, esta etapa puede realizarse con conjuntos de datos que no dispongan de caídas. Dado que por la naturaleza del modelo propuesto el entrenamiento se realiza únicamente con muestras de actividades cotidianas. A pesar de la existencia de Bases de Datos que cumplen los criterios, se decide finalmente optar por generar un conjunto propio. Al formar parte del desarrollo realizado, trataremos este punto más adelante, en \fullref{sec:imp:accelcapture}.




\warn{revisar esto}
Para poder generar un modelo con el mismo tipo de datos y capturado de la misma forma a los que deberá procesar posteriormente, optamos por realizar un base de datos de datos propio usando el mismo dispositivo sobre el que se ejecutará el prototipo como plataforma de captura.

Este base de datos se realiza usando únicamente información del sensor de aceleración triaxial del reloj Fossil Sport Gen3 \ref{app:sec:fossil} a una frecuencia de muestreo de 50Hz. Los valores de aceleración medidos están expresados en $m/s^2$ dentro de los límites de la plataforma. Este es un base de datos no etiquetado dado que el objetivo es entrenar un modelo de predicción de series temporales y no un clasificador como suele ser habitual.

Para generar las capturas de los acelerómetros se implementa la aplicación \accelcapture/ (\autoref{sec:imp:accelcapture}). En el  \autoref{chap:generar:dataset} se encuentra una descripción del proceso de generación y especificaciones detalladas del base de datos generado.
\iffalse
\todo{no ponerlo como apéndice, es parte importante, ponerlo aquí (referenciar ciertas cosas al apéndice que sean de menor interés)}
\fi
A la hora de parametrizar el sistema de captura de datos, optamos por mantener los 50Hz de frecuencia de muestreo. \cite{Liu2018} demuestra que se pueden conseguir buenos modelos basados en aprendizaje automático con tan solo 5,8Hz, aunque generaliza en 21,3Hz la frecuencia de muestreo mínima necesaria. Nos mantenemos por encima de este límite para eliminar una posible fuente de errores en el modelo.





\section{Modelos y Algoritmo}\label{sec:req:modelos}
\iffalse
\todo{nada de vaguedades, qué modelos, cuales voy a usar, qué valores he calculado yo y cuales vienen de fuera}
Estudios similares han mostrado la mayor capacidad de los sistemas basados en aprendizaje automático. Sin embargo estos algoritmos no han conseguido desplazar en los sistemas comerciales a los métodos analíticos por la gran capacidad de cálculo que requieren. Algunos trabajos han demostrado la eficacia de hibridar los modelos usando un modelo computacionalmente simple para realizar una tarea de detección de candidatos a caídas de fondo y un modelo complejo que actúe únicamente sobre este subconjunto de episodios. Este es el acercamiento que toma este trabajo.
\fi
\subsection{Modelos Analíticos}

En el apartado \ref{sec:arte:modelos_analiticos} hemos introducido el contexto de este conjunto de algoritmos. Para implementar el sistema necesitamos un modelo simple y de bajo coste computacional y que se base principalmente en los datos captados por un acelerómetro de tres ejes. Dada la situación del sensor, toda medida de la postura queda descartada por no ser aplicable. Finalmente, dado que su función es principalmente la de mecanismo de atención o generador de eventos de posibles caídas es primordial que su sensibilidad sea alta para no perder caídas. La tasa de especificidad modulará la eficiencia del sistema en los planos del tiempo de cálculo y consumo de energía, siendo preferible usar algoritmos con alta especificidad, siempre y cuando no se comprometa la sensibilidad. Tras analizar varios modelos y recopilar los resultados en la tabla \ref{tab:resumen_mod_anal} decidimos optar por la familia de algoritmos propuestos por \cite{Bourke2006} por ser el único que de forma consistente ofrece una sensibilidad del 100\%. Si se observan los resultados de los diferentes algoritmos analizados, se observa la gran varianza de resultados según el estudio que realizase el análisis. Por ejemplo, el algoritmo \textit{Bourke 1} pasa de ser un algoritmo perfecto con una precisión del 100\% a ser inservible por su bajísima especificidad según el conjunto de datos usado.

\tablan[0.95\linewidth]{tab:resumen_mod_anal}{Características de los modelos analíticos estudiados}{lccccccccll}{
          &       &     & \multicolumn{2}{c}{\emph{Estudio 1\tnote{1}}} &\multicolumn{2}{c}{\emph{Estudio 2\tnote{2}}} & \multicolumn{2}{c}{\emph{Estudio 3\tnote{3}}} & \\
 {\small Modelo}  & {\small Sens.\%} & {\small Esp.\%} & {\small Sens.\%} & {\small Esp.\%} & {\small Sens.\%} & {\small Esp.\%} & {\small Sens.\%} & {\small Esp.\%} & {\small Sensores } & {\small Postura} \\ \midrule
 Kangas 1\tnote{4}  & 100 & -- & 48 & 94 &   &    &   &  & ACC & Si \\
 Kangas 2           & 100 & -- & 48 & 94 & 86 & 98 &   &  & ACC & Si \\
 Kangas 3           & 71  & -- & 31 & 97 & 94 & 94 &   &  & ACC & Si \\
 Bourke 1\tnote{5}  & 100 & 100& 100& 19 & 100& 79 &   &  & ACC & No \\
 Bourke 2\tnote{6}  & 97  & 99 & 72 & 87 & 70 & 99 &   &  & ACC & Si \\
 Bourke 3\tnote{7}  & 100 & 100& 87 & 97 &    &    &   &  & ACC + GYRO & Si \\
 Chen \tnote{8}     &     &    & 76 & 94 &    &    &   &  & ACC & No \\ % ACC & Si
 FallIndex\tnote{9} &     &    &    &    &    &    & 42&95& ACC & No \\
}{
  \item [1] \citeA{Bagala2012}
  \item [2] \citeA{Aziz2017}
  \item [3] \citeA{Vilarinho2015}
  \item [4] \citeA{Kangas2008}
  \item [5] \citeA{Bourke2006}
  \item [6] \citeA{Bourke2008}
  \item [7] \citeA{Bourke2010}
  \item [8] \citeA{Chen2005}
  \item [9] \citeA{fallindex00}
  }{3}



\paragraph{Algoritmo Bourke 1} El modelo de Bourke 1\cite{Bourke2006} se basa en el estudio de la evolución del módulo de la aceleración. Partimos del principio de que todo cuerpo sufre una aceleración constante de $9,8m/s$ o 1G correspondiente a la gravedad terrestre. Como podemos apreciar en la figura \ref{fig:bourke_thresholds}, al principio de una caída, típicamente se produce un descenso en el módulo de la aceleración ya que al caer, la componente vertical contrarresta el efecto de la gravedad. Al impactar posteriormente contra el suelo se produce un pico en la aceleración. El algoritmo de Bourke utiliza esta característica forma para establecer dos niveles para determinar la detección de caídas. Estas cotas se denominan \textit{LFT} (Nivel de caída inferior) para el nivel mínimo de aceleración medido y \textit{UFT} (Nivel de caída superior) para el nivel superior. El algoritmo de Bourke considera que se ha producido una caída cuando en un intervalo determinado de tiempo se rebasan ambos límites.

El proceso para elegir estos límites garantiza que por construcción la sensibilidad del algoritmo sea del 100\% sobre la base de datos elegida. Para ambos límites, se recorre todo el conjunto de datos y se busca entre todas las caídas:
\begin{itemize}
  \item El mayor de los mínimos de aceleración de las caídas (para definir \textbf{LFT})
  \item El menor de los máximos de aceleración de las caídas (para definir \textbf{UFT})
\end{itemize}

Garantizando así que todas las caidas del conjunto de datos serán detectadas, y por tanto la sensibilidad siempre sea máxima. Por contra esto hace que el valor de especificidad del algoritmo sea altamente dependiente del resto de actividades incluidas en la base de datos. Esta característica explicaría la gran disparidad de resultados de la tabla \ref{tab:resumen_mod_anal}.

\figura{BourkeThresholds}{fig:bourke_thresholds}{Detalle del modelo de detección de Bourke}

Recogemos en la tabla \ref{tab:bourke_threshold_values} una lista con los diferentes valores obtenidos para LFT y UFT según diversos estudios. Su valor es claramente orientativo dada la dependencia de la base de datos usada, que nos obligará a obtener estos valores nuevamente durante la implementación para adaptarlos a SisFall.

\tabla[0.43\linewidth]{tab:bourke_threshold_values}{Valores UFT y LFT de Bourke por estudio}{llcc}{
  \emph{Estudio} & \emph{Posición} & \emph{UFT} & \emph{LFT} \\ \midrule
  \citeA{Bourke2006} & Pecho & 3,52g & 0,41g \\
                     & Cintura & 2,74g & 0,60g \\
  \citeA{Bourke2010} & Cintura & 2,80g & 0,65g \\
  \citeA{Bagala2012} & -- & 1,79g & 0.73g \\
}{2}


\subsection{Modelos Basados en Aprendizaje Automático}

\subsection{Modelos Híbridos}

\section{Biblioteca de Modelos Neuronales} \label{sec:req:tflite}

Una de las piezas angulares de todo el desarrollo es la generación y entrenamiento del modelo. Si bien es factible realizar una imlementación directa tanto del modelo como de las funciones de entrenamiento, no es óptimo ni recursos ni en resultados. Existen multitud de de bibliotecas para el desarrollo, entrenamiento y despliegue de modelos de los que citamos brevemente los más extendidos:

\paragraph{Caffe2 y PyTorch}

Caffe2 (\url{https://caffe2.ai/}) Es en su origen una biblioteca de modelos neuronales centrada en la velocidad con soporte para el lenguaje de programación \textit{python}. Desde hace 2018 forma parte de la biblioteca PyTorch (\url{https://pytorch.org/}). PyTorch auna un módulo de cálculo de tensores y uno de modelado de redes neuronales con gran énfasis en la aceleración mediante GPU. Es, junto con TensorFlow, la biblioteca de Modelos Neuronales más usada en el ámbito académico.

\paragraph{TensorFlow + Keras}

Similar al caso anterior, Keras (\url{https://keras.io/}) es un módulo que ofrece una API de alto nivel para facilitar la generación y entrenamiento de modelos. No es en si una biblioteca de redes neuronales sino una interfaz de alto nivel a varias de estas como TensorFlow, Theano, CNTK y otros. TensorFlow es al igual que PyTorch una biblioteca de cálculo de tensores y de modelado de redes si bien sus algoritmos están optimizados para ejecutarse sobre unidades TPU también soportan aceleración en GPUs. Desde la versión 2.0 de TensorFlow, se incluye Keras para ofrecer tanto una API de bajo nivel como otra de alto para la generación de modelos. Al ser propiedad de Google, la integración con sus productos es de las mejores. Dispone de un módulo para la ejecución de modelos en plataformas embebidas.

\paragraph{SciKit-Learn}

Scikit-learn (\url{https://scikit-learn.org/}) se presenta como una biblioteca de funciones de tratamiento de datos. Ofrece imlementaciones de funciones y algoritmos de aprendizaje automático muy variados como \textit{random forest}, \textit{SVM}, textit{K-NN} y otros, así como funciones para el pre-procesado de datos. Soporta también modelos de redes neuronales, aunque su API de desarrollo des de muy alto nivel y dificilmente industrializable.

La elección del sistema WearOS para imlementar la aplicación, unida a la popularidad tanto en el ámbito académico como profesional así como el soporte y optimización para sistemas embebidos hace que nos decantemos por la opción TensorFlow + Keras.

\section{Optimización de modelos}\label{sec:req:optimizacion}

Los modelos basados en redes neuronales adolecen generalmente por su gran tamaño y la necesidad realizar infinidad de cálculos. Estos problemas se acrecentan cuando el objetivo es usar el modelo en un sistema embebido con poca memoria y microprocesadores optimizados para el consumo de energía en lugar de potencia de cálculo. Esta escasez de recursos obliga a la \textit{optimización} del algoritmo y modelo usados. Entendiendo por optimización la reducción de complejidad controlada para conseguir un modelo que requiera menor capacidad de cálculo y memoria. Estas técnicas tendrán todas un impacto en la precisión del sistema por lo que el objetivo es conseguir un equilibrio entre la degradación de la capacidad predictiva y la reducción de la latencia debida al menor coste computacional. Establecemos dos criterios previos a cumplir por el sistema final:

\begin{itemize}
  \item Arbitrariamente escogemos una latencia máxima de 1 segundo desde el instante del primer impacto.
  \item La elección de la plataforma nos limita tanto en velocidad de cálculo como en tamaño al disponer únicamente de 512MBytes de memoria RAM.
\end{itemize}

\subsection{Técnicas de Optimización: Pruning y Cuantificación}

  A pesar de la creciente capacidad de cálculo de los dispositivos vestibles, será necesario optimizar el rendimiento del modelo con el fin de conseguir la mayor capacidad de predicción por Hz del sistema. Para ello dispondremos de dos técnicas: \textit{Pruning} y \textit{Cuantificación}.

\subsubsection{Pruning}

Si añadimos que indican la contribción de cada nodo o neurona al resultado final. Es normal que en esa red haya nodos que aporten más información que otros. Este efecto no es necesariamente negativo, la regularización o normalización L1 busca precisamente este efecto para reducir el sobreajuste de la red a los datos de entrenamiento.


Si pensamos en reducir el tamaño de una red neuronal, probablemente una de las soluciones que nos venga a la cabeza sea sea hacer un \textit{tree-shacking}, eliminar aquello que no se necesita. El término, usado en ciencias de la computación se aplica para reducir el tamaño de aplicaciones al eliminar el código que no es utilizado. Podemos pensar en realizar un cribado similar en la red eliminando aquellos pesos menos relevantes para la salida. Propuesto en \citeA{Mozer1989} como una herramienta para simplificar las redes de neuronas y facilitar así la comprensión de su funcionamiento, acelerar el aprendizaje al reducir el tamaño de la red y por la misma razón facilitar la capacidad de la red de generalizar. La técnica propuesta consiste en medir la influencia de los nodos en la señal de salida: comparar el resultado con y sin el nodo activado. Sabiendo la influencia de la aportación de cada nodo podemos determinar un umbral y eliminar los nodos con menor contribución reduciendo la red. \citeA{Yann1989} confirma que además de conseguir reducir el tamaño de la red a la mitad, el aprendizaje de la misma puede llegar a mejorar.


Recordemos, una capa de una red neuronal se puede modelar mediante la ecuación $H_i = f_i(W_ix+B_i)$ donde tenemos una matriz de conexiones $W_{i}$ y una matriz de sesgos $B_{i}$ y la función de activación $f_i(x)$. Si añadimos varias capas sucesivas podemos deducir que el impacto de un nodo, especialmente en las primeras capas de la red crecerá con cada capa posterior. Sin embargo eso no es así ya que en muchos casos la aportación puede quedar neutralizada por sucesivas inversiones de signo en el resto de capas, o resultar que el nodo se activa prácticamente con el mismo valor o que las capas posteriores estén en el límite de la función de activación y su influencia sea prácticamente irrelevante\cite{Mozer1989}. Es más, posiblemente el lector haya rápidamente asociado por su similitud esta técnica con una regularización mediante \textit{dropout}. Las similitudes son evidentes, pero hay sin embargo diferencias importantes:
\begin{itemize}
  \item Los nodos anulados o eliminados se pierden de forma definitiva
  \item Los nodos se seleccionan de acuerdo a su aportación y no de forma aleatoria
\end{itemize}

A la hora de aplicar una técnica de pruning sobre la red es necesario entender que tras cada etapa de eliminación de nodos debe realizarse una etapa de reajuste de la red resultante para corregir el efecto de la pérdida de las neuronas. Este reajuste o aplanamiento de la red resulta especialmente efectivo en redes con múltiples capas como demuestra \citeA{Cai2020} logrando reducir el tamaño de \textit{ResNet110} en un 30\% y un descenso de la precisión de la red del 0,15\% y la reducción del número de operaciones necesarias a la mitad. Otros estudios\cite{Lee2019,Bartoldson2019,Han2015} confirman estos resultados y presentan una novedad: Las grandes reducidas mediante pruning presentan mejores resultados que los modelos pequeños (ResNet32 tras ser comprimida es mejor clasificando imágenes que WRN16, a igual número de nodos). Básicamente, se mantiene la mayor capacidad de generalización o aprendizaje del uso de mayor número de capas incluso al reducir el número de neuronas.

Esta técnica, junto con la cuantificación de los valores, se presenta como una herramienta imprescindible para lograr los objetivos de nuestro sistema. Tensorflow permite aplicar prunning a los modelos, aunque el único algoritmo disponible, \textit{prune low magnitude} sea similar al básico definido por \citeA{Mozer1989}: eliminar aquellos nodos cuyo peso sea menor que un determinado nivel.
\[
  pruned(w_{i})=\left\{
    \begin{array}{lcl}
      w_i & si & |w_i| > \lambda \\
      0 & si & |w_i|\leq  \lambda
    \end{array}
    \right.
\]

Este proceso se va realizando durante un número determinado de iteraciones de entrenamiento durante el cual se van anulando progresivamente una cantidad determinada (ya sea un número fijo o una evolución polinomial) de nodos en cada etapa de entrenamiento. \cite{Yann1989} Los modelos son cada vez más comlejos y con el aumento en complejidad de problemas seguirán crecienco. Un método que elimina nodos permite no solo mejorar la capacidad de generalizar

\iffalse

  https://www.machinecurve.com/index.php/2020/09/23/tensorflow-model-optimization-an-introduction-to-pruning/
  ligeramente más técnico.
https://www.machinecurve.com/index.php/2020/09/29/tensorflow-pruning-schedules-constantsparsity-and-polynomialdecay/
\fi


\subsubsection{Cuantificación}

Además de eliminando nodos de la red, hay otra técnica que permite reducir el tamaño y requisitos de esta: reducir el número de bits usados para representar las conexiones y sesgos. Entendemos por cuantificación al proceso de discretizar un grupo de valores. En este caso en concreto hace referencia al mencionado descenso en la precisión numérica usada para representar los pesos de nodos y valores de entrada y salida de una red neuronal.

Una de las primeras aproximaciones a la cuantificación de los parámetros de redes neuronales aparece en \citeA{Courbariaux2015}. \citeauthor{Courbariaux2015} busca una solución al problema de los crecientes requisitos de potencia de cálculo necesarios por los modelos de redes de neuronas. Esta optimización espera reducir la dependencia en la mejora de las unidades GPU(\textit{Graphics Processing Unit}) y CPU, hasta ahora uno de los principales factores que limitan evolución en este área. \cita[{\citeNP[p.~1]{Courbariaux2015}}]{La mayoría de los cálculos realizados en el entrenamiento y aplicación de una red neuronal consisten en multiplicar un peso determinado por valor real por una activación definida por un valor real}, así pues decide binarizar tanto pesos como activaciones y reducirlos a solo dos valores: $0$ y $1$ o $+1$ y $-1$. Los resultados obtenidos son esperanzadores, una reducciónd el tamaño a 1/16 del original (de 16 bits usados para representar pesos y activaciones pasa a uno único) y una mejora en la velocidad de respuesta de la red tres veces menor.

La reducción en el tamaño de la red es evidente. Si tenemos $N$ parámetros en una red representados con $m$ bits cada uno, el tamaño final de la red es $S(red)=Nm$ es por tanto una dependencia lineal. La mejora en los tiempos de cómputo es más complejo. Trabajar con números reales, o de coma flotante, requiere de unidades de cálculo especializadas, ya sean GPUs o unidades de coma flotante en CPUs. Este requisito de unidades especializadas es determinante en el rendimiento, pues será determinante en la velocidad de cálculo del modelo. Esta problemática la estudia en detalle \citeA{Vanhoucke2011} buscando optimizar la velocidad de los modelos. Se fija en la lentitud de las unidades CPU para realizar grandes cantidades de cálculos en coma flotante y propone en este caso usar valores enteros. Predice que el impacto negativo será mínimos pues \cita[{\citeNP[p.~5]{Vanhoucke2011}}]{las activaciones son probabilidades, limitadas al rango $[0:1]$ así pues no hay riesgos relacionados con el escalado al usar enteros. Y como la entrada de cada nodo es la salida de una activación esto asegura que el rango de valores permanecerá acotado}. Aplicando cuantización para convertir los pesos de reales a enteros, y otras optimizaciones en la implementación, consigue reducir a menos de un tercio el tiempo de respuesta de una red de procesado del habla con un error de precisión añadido inferior al 0,1\%.

Hasta ahora hemos tratado pesos y activaciones como un todo: aplicamos la misma regularización o reducción de precisión a ambos conjuntos de valores por igual. A esta técnica se la conoce como \textit{Cuantificación de precisión uniforme}, en contraste con la \textit{Cuantificación de precisión mixta}. Este última propone tratar la reducción de tamaño de cada conjunto de valores por separado, consiguiendo una mayor reducción del tamaño con un impacto menor en la precisión del modelo \cite{Jin2019} e incluso a veces mejorando el rendimiento.

El resultado del último párrafo, de nuevo, no habrá pillado desprevenido al lector que rápidamente asocia la reducción en la precisión con un aumento del ruido, al que llamamos \textit{ruido de cuantificación} y por tanto similar a una regularización de la red por ruido \cite{Bishop1995,Noh2017}. Esta capacidad de regularización de la red resultante de la cuantificación y entrenamiento de los pesos de la red es utilizado directamente por \citeA{Wu2019} no ya con el objetivo de mejorar el tamaño y velocidad del modelo sino para mejorar su capacidad de aprendizaje y generalización mejorando la capacidad de clasificación de imágenes de una red en hasta un 10\%.

TensorFlow Lite ofrece diversas opciones para realizar la cuantificación de modelos como presenta la tabla \ref{tab:tflite_quantization}. En resumidas cuentas ofrece dos opciones: realizar la cuantificación sobre modelos ya entrenados o realizarlo durante el entrenamiento del modelo. Este segundo tipo ofrece mejor granularidad y fiabilidad ya que permite definir secciones específicas del modelo que no sean alteradas y asegura que los rangos y escalados de se realizan teniendo el cuenta todos los datos disponibles.


\tablan{tab:tflite_quantization}{Detalle de los tipos de cuantificación disponibles en TensorFlow Lite}{llcccl}{
  Técnica                   & Datos Requeridos  & Compresión  & Precisión & Velocidad & Hardware Soportado \\ \hline
  Float16\tnote{1}          & -                 & \~50\%      & 0         & --        & CPU, GPU \\
  Rango Dinámico\tnote{1}   & -                 & \~75\%      & ---       & x2,5      & CPU, GPU \\
  Enteros\tnote{1}          & Si\tnote{3}       & \~75\%      & --        & x3        & CPU, GPU, TPU, DSP \\
  QAT\tnote{2}              & Si\tnote{4}       & \~75\%      & -         & x3        & CPU, GPU, TPU, DSP \\
}{
  \item [1] Cuantificación post-entrenamiento.
  \item [2] \textit{Quantification Aware Training} (Entrenamiento Para Cuantificación), cuantificación se prepara durante el entrenamiento.
  \item [3] Es necesario proveer un subconjunto representativo de datos.
  \item [4] Se usa todo el conjunto de entrenamiento.
  }{3}

La precisión y número de bits usado para representar los pesos y activaciones puede verse en la tabla \ref{tab:tflite_weights_activations}. La cuantificación a enteros y OAT resultan en redes con el mismo peso, la diferencia entre ambos es cómo se calculan y escalan los valores (Enteros usa un subconjunto de datos, lo cual penaliza su precisión, contra el uso de toda la base de datos de entrenamiento como hace OAT), y en que OAT al realizarse durante el entrenamiento permite compensar el error introducido, funcionando, como ya hemos mencionado, como una regularización.

\tablan[0.55\linewidth]{tab:tflite_weights_activations}{Tipos de representación usados en cada técnica}{lcc}{
  Técnica & Pesos & Activaciones \\ \hline
  Float16 & Float(16bit) & Float(16bit) \\
  Rango Dinámico & Entero(8bit) & Entero(8bit)\tnote{1} \\
  Enteros & Entero(8bit) & Entero(8bit) \\
  Enteros Experimental & Entero(8bit) & Entero(16bit) \\
  OAT     & Entero(8bit) & Entero(8bit) \\
}{
  \item [1] Las activaciones se convierten de coma flotante a entero al vuelo y el resultado se almacena en coma flotante
  }{2}

  La combinación de estas técnicas, no solo es posible, sino deseable para conseguir reducir el consumo de recursos, memoria, consumo energético y tiempo de proceso, en órdenes de magnitud. El uso de Pruning y Cuantificación permite reducir grandes redes a una fracción: VGG16 se comprime de los 552MBytes originales a apenas 11,3MBytes, Alex-Net adelgaza de 240MBytes hasta los 6,9MBytes ambas sin pérdida de precisión\cite{Han2015}. Respecto a los tiempos, según \citeauthor{Han2015}, en un chip integrado para móviles los tiempos de ejecución son en promedio 4,2 veces más rápidos. Dados estos resultados previos, esperamos que estas técnicas tengan un gran impacto a la hora de permitir la ejecución de modelos de calidad de manera eficiente.














La cuantificación de los valores de entrada y salida a la red consiste en reducir el peso de estos valores. Normalmente se normaliza la entrada y se convierte en valores de tipo entero. Tras el paso por la red, a la salida se aplicac la operación inversa para escalar la salida al rango de valores esperado.

Por su parte la cuantificación de la red neuronal realiza una operación similar con los pesos de los enlaces de las neuronas. Reduce la precisión de las representaciones de estos valores consiguiendo un ahorro en recursos y una mejora en tiempo de cálculo al eliminar el requisito de usar unidades de coma flotante en sistemas embebidos.




% \end{document}

